# SEngBERT
SEngBERT: A Pretrained Language Representation Model for the Software Engineering domain

This repository provides the replication package for SEngBERT, a software engineering language representation model designed for domain-specific text mining tasks such as named entity recognition, relation extraction, question answering, etc.

## Download
We provide five versions of pre-trained weights. Pre-training was based on the [original BERT code](https://github.com/google-research/bert) provided by Google, and training details are described in our paper. Currently available versions of pre-trained weights are as follows:

* **[SEngBERT-Base v1.0](https://danielrusso.org)** - based on BERT-base-Cased (same vocabulary)



## Installation

Sections below describe the installation and the fine-tuning process of SEngBERT based on Tensorflow 1 [to be completed]
